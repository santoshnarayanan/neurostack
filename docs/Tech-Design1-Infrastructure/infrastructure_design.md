# üèó NVIDIA AI Control Plane (NACP)

## Infrastructure & Runtime Technical Design

------------------------------------------------------------------------

# 1Ô∏è‚É£ Purpose

This document describes the **Infrastructure Foundation** of the NVIDIA
AI Control Plane (NACP).

It covers:

-   Host operating environment
-   GPU enablement architecture
-   WSL2 Linux bridge design
-   Docker runtime configuration
-   NVIDIA Container Toolkit integration
-   Execution flow for GPU inference
-   Rebuild & disaster recovery strategy

This document complements `technical_design.md`, which focuses on
Control Plane and orchestration logic.

------------------------------------------------------------------------

# 2Ô∏è‚É£ Infrastructure Philosophy

NACP follows a **Local-First, GPU-Native Architecture**.

Design goals:

-   Deterministic runtime behavior
-   Reproducible container execution
-   Hardware-aware optimization
-   Clear separation between host, kernel bridge, and container runtime
-   Minimal virtualization overhead

------------------------------------------------------------------------

# 3Ô∏è‚É£ Host Environment

## üñ• Operating System

-   Windows 11 (Primary Host)

## üéÆ GPU Hardware

-   NVIDIA RTX 3050 Ti (4GB VRAM)
-   CUDA-enabled
-   Tensor Cores active

## üß© NVIDIA Driver

-   Windows NVIDIA Driver (v512.78)
-   CUDA 11.6 host compatibility

The Windows driver remains the authoritative GPU driver.

------------------------------------------------------------------------

# 4Ô∏è‚É£ WSL2 Linux Bridge Architecture

## üêß WSL2 (Ubuntu 24.04)

WSL2 provides:

-   A lightweight Linux kernel
-   Direct GPU passthrough
-   CUDA compatibility layer
-   Shared driver integration with Windows host

WSL2 acts as a **Linux kernel bridge**, enabling Docker to run
Linux-native GPU workloads on Windows.

### Execution Flow

    Container (Ollama / PyTorch)
        ‚Üì
    Docker Engine
        ‚Üì
    NVIDIA Container Runtime
        ‚Üì
    WSL2 Linux Kernel
        ‚Üì
    Windows NVIDIA Driver
        ‚Üì
    RTX 3050 Ti GPU

WSL2 is not a virtual machine in the traditional sense.\
It provides near-native performance with minimal overhead.

------------------------------------------------------------------------

# 5Ô∏è‚É£ Docker Runtime Configuration

## üê≥ Docker Desktop (WSL Backend)

-   Uses WSL2 backend
-   GPU-enabled via NVIDIA Container Toolkit
-   Runtime registered: `nvidia`

Validation command:

    docker info | findstr -i nvidia

Expected output:

    Runtimes: io.containerd.runc.v2 nvidia runc

------------------------------------------------------------------------

# 6Ô∏è‚É£ NVIDIA Container Toolkit

Installed inside WSL2 Ubuntu.

Responsibilities:

-   Registers NVIDIA runtime
-   Enables `--gpus all` flag
-   Bridges Docker containers to host GPU

Verification:

    docker run --rm --gpus all nvidia/cuda:11.6.2-base-ubuntu20.04 nvidia-smi

------------------------------------------------------------------------

# 7Ô∏è‚É£ GPU Inference Lifecycle

When an LLM inference request is executed:

    User Request
        ‚Üì
    FastAPI (Control Plane)
        ‚Üì
    Ollama Container
        ‚Üì
    Docker NVIDIA Runtime
        ‚Üì
    WSL2 Kernel
        ‚Üì
    Windows NVIDIA Driver
        ‚Üì
    RTX 3050 Ti executes CUDA kernels
        ‚Üì
    Inference output returned to container
        ‚Üì
    Response returned to API

GPU is engaged only during model inference.

Database operations remain CPU-bound.

![GPU reference cycle](/docs/diagrams/phase3/gpu_inference_cycle.png)

------------------------------------------------------------------------

# 8Ô∏è‚É£ Containerization Strategy

All AI runtime components are containerized:

-   LLM runtime (Ollama)
-   Embedding service (future phase)
-   Graph reasoning adapters (future phase)

Container flags include:

-   `--gpus all`
-   `--restart unless-stopped`
-   Persistent volume mounting for model storage

Example deployment:

    docker run -d \
      --gpus all \
      -p 11434:11434 \
      --name ollama \
      --restart unless-stopped \
      -v ollama_data:/root/.ollama \
      ollama/ollama

------------------------------------------------------------------------

# 9Ô∏è‚É£ Parallel Execution Considerations

Infrastructure supports:

-   Concurrent CPU-based retrieval tasks
-   GPU-based inference tasks
-   Asynchronous orchestration from Control Plane

Resource contention is monitored via:

-   GPU utilization
-   VRAM usage
-   Token throughput

------------------------------------------------------------------------

# üîü Observability at Infrastructure Layer

Metrics available:

-   `nvidia-smi` GPU metrics
-   Docker container stats
-   Token/sec from LLM runtime
-   Latency per execution stage

Future integration planned:

-   Prometheus exporters
-   Structured metrics aggregation

------------------------------------------------------------------------

# 1Ô∏è‚É£1Ô∏è‚É£ Rebuild & Disaster Recovery Strategy

Infrastructure can be rebuilt via scripted steps:

## Windows Layer

-   Install NVIDIA Driver
-   Enable WSL2

## WSL2 Layer

-   Install Ubuntu
-   Install NVIDIA Container Toolkit
-   Configure Docker runtime

## Container Layer

-   Redeploy containers via Docker Compose
-   Restore volumes

This enables deterministic infrastructure recovery.

------------------------------------------------------------------------

# 1Ô∏è‚É£2Ô∏è‚É£ Current Infrastructure Status

  Layer                       Status
  --------------------------- ----------------
  Windows Host                ‚úÖ Stable
  NVIDIA Driver               ‚úÖ Active
  WSL2 GPU Bridge             ‚úÖ Operational
  Docker NVIDIA Runtime       ‚úÖ Registered
  CUDA Container Validation   ‚úÖ Passed
  PyTorch GPU Validation      ‚úÖ Passed

------------------------------------------------------------------------

# üèÅ Conclusion

The NACP infrastructure foundation provides:

-   Stable GPU-backed local inference
-   Minimal-overhead Linux bridge via WSL2
-   Containerized reproducibility
-   Deterministic execution path
-   Enterprise-ready separation between host, runtime, and orchestration
    layers

This infrastructure layer forms the compute backbone for:

-   Control Plane orchestration
-   Hybrid reasoning workflows
-   Retrieval-Augmented Generation
-   Graph reasoning extensions

------------------------------------------------------------------------

End of Infrastructure Technical Design Document
